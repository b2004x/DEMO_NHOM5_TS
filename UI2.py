import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import glob
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os


# ============== CONFIG ==================
# === Configuration ===
csv_files = sorted(glob.glob(r"G:\hoc\Time series\UI\data_chunks_2GB\*.csv"))[:30]
timesteps = 20
batch_size = 64
input_size = 8
store_id_list = ["C·ª≠a h√†ng 1","C·ª≠a h√†ng 2"]
family_ids_list = ["Deli","Eggs"]
store_id_Name = st.selectbox("Ch·ªçn store_id", options=store_id_list, )
family_id_Name = st.selectbox("Ch·ªçn family_id", options=family_ids_list)
if store_id_Name == "C·ª≠a h√†ng 1":
    store_id = 0.0377358490566037
elif store_id_Name == "C·ª≠a h√†ng 2":
    store_id = 0.0188679245283018
if family_id_Name == "Deli":
    family_id = 0.46875
elif family_id_Name == "Eggs":
    family_id = 0.6875
if st.button("Ti·∫øp t·ª•c"):
    st.success("ƒê√£ ch·ªçn xong!")
    st.write("‚úÖ Store ID:", store_id_Name)
    st.write("‚úÖ Family ID:", family_id_Name)


    # === Fit MinMaxScaler globally on all data (first 100k rows) ===
    df_raw = pd.read_csv(csv_files[0]).dropna()
    feature_cols = ['onpromotion_scaled', 'year_scaled', 'month_sin', 'month_cos',
                    'day_sin', 'day_cos', 'family_scaled', 'store_scaled']
    target_col = 'sales_scaled'

    scaler = MinMaxScaler().fit(df_raw[feature_cols])
    # ============== MODEL ==================
    # === Dataset ===
    class ChunkedTimeSeriesDataset(Dataset):
        def __init__(self, csv_files, timesteps, feature_cols, target_col, scaler, store_id, family_id):
            self.csv_files = csv_files
            self.timesteps = timesteps
            self.feature_cols = feature_cols
            self.target_col = target_col
            self.buffer = []
            self.file_idx = 0
            self.chunk_iter = None
            self.scaler = scaler
            self.store_id = store_id
            self.family_id = family_id
            self._load_next_chunk()

        def _load_next_chunk(self):
            while self.file_idx < len(self.csv_files):
                file = self.csv_files[self.file_idx]
                self.chunk_iter = pd.read_csv(file, chunksize=100_000)
                self.file_idx += 1
                try:
                    chunk = next(self.chunk_iter).dropna()
                    chunk = chunk[(chunk['store_scaled'] == self.store_id) &
                                (chunk['family_scaled'] == self.family_id)]
                    if chunk.empty:
                        continue
                    scaled_features = self.scaler.transform(chunk[self.feature_cols])
                    targets = chunk[self.target_col].values.reshape(-1, 1)
                    self.buffer = np.hstack((targets, scaled_features)).tolist()
                except StopIteration:
                    continue

        def __len__(self):
            return int(1e5)

        def __getitem__(self, idx):
            while len(self.buffer) < self.timesteps + 1:
                self._load_next_chunk()
                if len(self.buffer) < self.timesteps + 1:
                    return self.__getitem__(idx + 1)

            start = np.random.randint(0, len(self.buffer) - self.timesteps - 1)
            window = np.array(self.buffer[start:start + self.timesteps + 1])
            return torch.tensor(window[:-1, 1:], dtype=torch.float32), torch.tensor(window[-1, 0], dtype=torch.float32)

    # === TCN Model ===
    class Chomp1d(nn.Module):
        def __init__(self, chomp_size):
            super().__init__()
            self.chomp_size = chomp_size

        def forward(self, x):
            return x[:, :, :-self.chomp_size].contiguous()

    class TemporalBlock(nn.Module):
        def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout=0.2):
            super().__init__()
            self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,
                                stride=stride, padding=padding, dilation=dilation)
            self.chomp1 = Chomp1d(padding)
            self.relu1 = nn.ReLU()
            self.dropout1 = nn.Dropout(dropout)

            self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,
                                stride=stride, padding=padding, dilation=dilation)
            self.chomp2 = Chomp1d(padding)
            self.relu2 = nn.ReLU()
            self.dropout2 = nn.Dropout(dropout)

            self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,
                                    self.conv2, self.chomp2, self.relu2, self.dropout2)
            self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None
            self.relu = nn.ReLU()

        def forward(self, x):
            out = self.net(x)
            res = x if self.downsample is None else self.downsample(x)
            return self.relu(out + res)

    class TCNModel(nn.Module):
        def __init__(self, input_size, num_channels, kernel_size=3, dropout=0.2):
            super().__init__()
            layers = []
            for i in range(len(num_channels)):
                dilation_size = 2 ** i
                in_channels = input_size if i == 0 else num_channels[i - 1]
                out_channels = num_channels[i]
                layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,
                                        dilation=dilation_size, padding=(kernel_size - 1) * dilation_size,
                                        dropout=dropout)]
            self.network = nn.Sequential(*layers)
            self.linear = nn.Linear(num_channels[-1], 1)

        def forward(self, x):
            x = x.permute(0, 2, 1)
            out = self.network(x)
            out = out[:, :, -1]
            return self.linear(out)

    # ============== STREAMLIT UI ==================
    st.set_page_config(page_title="üìà D·ª± b√°o v·ªõi m√¥ h√¨nh TCN", layout="centered")
    st.title("üîÆ D·ª± ƒëo√°n doanh s·ªë 7 ng√†y t·ªõi b·∫±ng m√¥ h√¨nh TCN")

    device = torch.device("cpu")

    dataset = ChunkedTimeSeriesDataset(csv_files, timesteps, feature_cols, target_col,
                                        scaler, store_id, family_id)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

    model = TCNModel(input_size=input_size, num_channels=[64, 64, 64], kernel_size=3, dropout=0.2).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    st.success("‚úÖ ƒê√£ l·ªçc d·ªØ li·ªáu theo store/family.")

    model = TCNModel(input_size=input_size, num_channels=[64, 64, 64], kernel_size=3, dropout=0.2)

    # 2. Load tr·ªçng s·ªë t·ª´ file ƒë√£ l∆∞u
    model.load_state_dict(torch.load(rf"G:\hoc\Time series\UI\Models\tcn_model_store{store_id}_family{family_id}.pth", map_location=device))
    
    # 3. Chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë√°nh gi√° v√† ƒë√∫ng thi·∫øt b·ªã
    model.to(device)
    model.eval()


    # === Evaluation ===
    st.subheader("üìâ Bi·ªÉu ƒë·ªì D·ª± ƒëo√°n vs Th·ª±c t·∫ø")
    y_true, y_pred = [], []
    with torch.no_grad():
        for i, (X, y) in enumerate(loader):
            X, y = X.to(device), y.to(device)
            outputs = model(X).squeeze()
            y_true.extend(y.cpu().numpy())
            y_pred.extend(outputs.cpu().numpy())
            if i >= 100:
                break

    min_y, scale_y = scaler.min_[0], scaler.scale_[0]
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    y_pred = y_pred * scale_y + min_y
    y_true = y_true * scale_y + min_y

    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mse)

    print("\n\U0001F4CA Evaluation Metrics:")
    print(f"MAE  = {mae:.4f}")
    print(f"RMSE = {rmse:.4f}")
    print(f"R¬≤   = {r2:.4f}")


    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(y_true, label="Th·ª±c t·∫ø", color="blue")
    ax.plot(y_pred, label="D·ª± ƒëo√°n", color="orange", linestyle="--")
    ax.legend()
    ax.set_title("So s√°nh D·ª± ƒëo√°n v√† Th·ª±c t·∫ø")
    st.pyplot(fig)

    st.markdown(f"""
    **üìä Metrics:**
    - MAE: `{mae:.2f}`
    - RMSE: `{rmse:.2f}`
    - R¬≤: `{r2:.2f}`
    """)

    # === Forecast 7 ng√†y t·ªõi ===
    forecast_steps = 7
    initial_sequence = [dataset.buffer[i][1:] for i in range(len(dataset.buffer) - timesteps, len(dataset.buffer))]
    input_seq = torch.tensor([initial_sequence], dtype=torch.float32).to(device)

    multi_preds = []
    model.eval()
    with torch.no_grad():
        for _ in range(forecast_steps):
            pred = model(input_seq).squeeze().cpu().item()
            multi_preds.append(pred)
            last_known_features = input_seq[0, -1, :].cpu().numpy()
            new_row = last_known_features.copy()
            input_seq = input_seq.cpu().numpy()
            input_seq = np.append(input_seq[0], [new_row], axis=0)[-timesteps:]
            input_seq = torch.tensor([input_seq], dtype=torch.float32).to(device)

    multi_preds = np.array(multi_preds)
    multi_preds_rescaled = multi_preds * scale_y + min_y

    true_last_30 = [row[0] for row in dataset.buffer[-30:]]
    true_last_30 = np.array(true_last_30) * scale_y + min_y

    combined = np.concatenate([true_last_30, multi_preds_rescaled])
    steps = np.arange(1, len(combined) + 1)

    fig, ax = plt.subplots(figsize=(14, 5))
    ax.plot(steps[:30], true_last_30, label="Th·ª±c t·∫ø (30 ng√†y)", color="blue", linewidth=2)
    ax.plot(steps[30:], multi_preds_rescaled, label="D·ª± ƒëo√°n (7 ng√†y)", color="orange", linestyle="--", linewidth=2)
    ax.axvline(x=30, color='gray', linestyle=':', label='B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n')
    ax.set_title(f"üìà D·ª± ƒëo√°n 7 ng√†y t·ªõi (Store {store_id} - Family {family_id})")
    ax.set_xlabel("Ng√†y")
    ax.set_ylabel("Sales")
    ax.legend()
    ax.grid(True)
    st.pyplot(fig)

    # Hi·ªÉn th·ªã s·ªë
    st.markdown("### üìä Gi√° tr·ªã th·ª±c t·∫ø (30 ng√†y g·∫ßn nh·∫•t):")
    st.write(np.round(true_last_30, 2))

    st.markdown("### üîÆ D·ª± ƒëo√°n doanh s·ªë 7 ng√†y t·ªõi:")
    for i, val in enumerate(multi_preds_rescaled, 1):
        st.write(f"Ng√†y +{i}: **{val:.2f}**")
# T·∫°o th∆∞ m·ª•c l∆∞u n·∫øu ch∆∞a c√≥


    base_dir = r"G:\hoc\Time series"
    save_dir = os.path.join(base_dir, f"results", f"store_{store_id}_family_{family_id}")
    os.makedirs(save_dir, exist_ok=True)

    # --- L∆∞u bi·ªÉu ƒë·ªì d·ª± ƒëo√°n vs th·ª±c t·∫ø ---
    fig1, ax1 = plt.subplots(figsize=(10, 4))
    ax1.plot(y_true, label="Th·ª±c t·∫ø", color="blue")
    ax1.plot(y_pred, label="D·ª± ƒëo√°n", color="orange", linestyle="--")
    ax1.legend()
    ax1.set_title("So s√°nh D·ª± ƒëo√°n v√† Th·ª±c t·∫ø")
    fig1.savefig(os.path.join(save_dir, "eval_plot.png"))

    # --- L∆∞u bi·ªÉu ƒë·ªì 30 ng√†y th·ª±c t·∫ø + 7 ng√†y d·ª± ƒëo√°n ---
    fig2, ax2 = plt.subplots(figsize=(14, 5))
    ax2.plot(steps[:30], true_last_30, label="Th·ª±c t·∫ø (30 ng√†y)", color="blue", linewidth=2)
    ax2.plot(steps[30:], multi_preds_rescaled, label="D·ª± ƒëo√°n (7 ng√†y)", color="orange", linestyle="--", linewidth=2)
    ax2.axvline(x=30, color='gray', linestyle=':', label='B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n')
    ax2.set_title(f"D·ª± ƒëo√°n 7 ng√†y t·ªõi (Store {store_id} - Family {family_id})")
    ax2.set_xlabel("Ng√†y")
    ax2.set_ylabel("Sales")
    ax2.legend()
    ax2.grid(True)
    fig2.savefig(os.path.join(save_dir, "forecast_plot.png"))

    # --- L∆∞u metrics ---
    metrics = {
        "MAE": float(mae),
        "RMSE": float(rmse),
        "R2": float(r2)
    }
    with open(os.path.join(save_dir, "metrics.json"), "w") as f:
        json.dump(metrics, f, indent=4)

    # --- L∆∞u d·ªØ li·ªáu ---
    np.save(os.path.join(save_dir, "true_last_30.npy"), true_last_30)
    np.save(os.path.join(save_dir, "forecast_next_7.npy"), multi_preds_rescaled)